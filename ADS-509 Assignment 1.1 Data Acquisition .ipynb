{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b4cd43",
   "metadata": {},
   "source": [
    "# ADS 509 Module 1: APIs and Web Scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f044608",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "These imports are used for file management, HTTP requests, HTML parsing, \n",
    "and safe scraping practices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "import requests\n",
    "import time   # <--- this one is required\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53728d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e525e71",
   "metadata": {},
   "source": [
    "# Artist Selection\n",
    "We selected two artists (The Who and Rush) with at least 20 available songs each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4de754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Lyrics\n",
    "artists = {\n",
    "    \"the_who\": \"https://www.azlyrics.com/w/who.html\",\n",
    "    \"rush\":    \"https://www.azlyrics.com/r/rush.html\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e592796",
   "metadata": {},
   "source": [
    "## Part 1: Robots.txt\n",
    "Q: Take a look at the `robots.txt` page on www.azlyrics.com. Is the scraping we are about to do allowed?  \n",
    "A: The `robots.txt` file does not explicitly disallow scraping lyrics pages. Out of caution, \n",
    "I implemented polite rate limiting (5–15s delay) and a custom User-Agent to minimize load.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c4a22",
   "metadata": {},
   "source": [
    "## Part 1: Harvesting Lyrics Links\n",
    "This code requests each artist’s main page, extracts all song links, \n",
    "and stores them in `lyrics_pages`. We also deduplicate the links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1451eee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_who: 200 lyric links found\n",
      "rush: 171 lyric links found\n"
     ]
    }
   ],
   "source": [
    "# Link Repo\n",
    "lyrics_pages = defaultdict(list)\n",
    "url_stub = \"https://www.azlyrics.com\"\n",
    "\n",
    "for artist, artist_page in artists.items():\n",
    "    r = requests.get(artist_page, headers=HEADERS, timeout=30)   # <- add headers+timeout\n",
    "    time.sleep(5 + 10*random.random())\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"../lyrics/\") or href.startswith(\"/lyrics/\"):\n",
    "            full_link = requests.compat.urljoin(url_stub + \"/\", href.replace(\"..\", \"\"))\n",
    "            lyrics_pages[artist].append(full_link)\n",
    "\n",
    "    lyrics_pages[artist] = list(set(lyrics_pages[artist]))        # <- dedupe here\n",
    "    print(f\"{artist}: {len(lyrics_pages[artist])} lyric links found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce26f75",
   "metadata": {},
   "source": [
    "### Checking Minimum Song Requirement\n",
    "This ensures each artist has at least 20 songs before scraping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d9abb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enough Songs\n",
    "for artist, lp in lyrics_pages.items():\n",
    "    assert len(set(lp)) >= 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d538f723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the_who we have 200.\n",
      "The full pull will take for this artist will take 0.56 hours.\n",
      "For rush we have 171.\n",
      "The full pull will take for this artist will take 0.47 hours.\n"
     ]
    }
   ],
   "source": [
    "# Scrape Time\n",
    "for artist, links in lyrics_pages.items() : \n",
    "    print(f\"For {artist} we have {len(links)}.\")\n",
    "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb599c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename_from_link(link):\n",
    "    if not link:\n",
    "        return None\n",
    "    name = link\n",
    "    name = name.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    name = name.replace(\".html\", \"\")\n",
    "    name = name.replace(\"/lyrics/\", \"\")\n",
    "    name = name.replace(\"://\", \"_\").replace(\".\", \"_\").replace(\"/\", \"_\")\n",
    "    return name + \".txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "510476f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lyrics Folder\n",
    "if os.path.isdir(\"lyrics\") : \n",
    "    shutil.rmtree(\"lyrics/\")\n",
    "\n",
    "os.mkdir(\"lyrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81426aa3",
   "metadata": {},
   "source": [
    "## Part 2: Scraping Lyrics\n",
    "This loop visits each lyrics page, extracts the title and lyrics, \n",
    "and saves them to `lyrics/<artist>/<filename>.txt`. \n",
    "Each file is written with the format:  \n",
    "*First line = title*  \n",
    "*Second line = blank*  \n",
    "*Remaining lines = lyrics*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d39fc9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping the_who... (200 links found)\n",
      "Saved 20 files for the_who\n",
      "Scraping rush... (171 links found)\n",
      "Saved 20 files for rush\n",
      "\n",
      "Scraping complete: 40 files written.\n",
      "Total runtime: 7.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Scraping\n",
    "start = time.time()\n",
    "total_pages = 0\n",
    "\n",
    "for artist, links in lyrics_pages.items():\n",
    "    artist_folder = os.path.join(\"lyrics\", artist)\n",
    "    os.makedirs(artist_folder, exist_ok=True)\n",
    "    print(f\"Scraping {artist}... ({len(links)} links found)\")\n",
    "\n",
    "    saved = 0\n",
    "    for link in links:\n",
    "        try:\n",
    "            # 1) request page (use headers + timeout; sleep after)\n",
    "            r = requests.get(link, headers=HEADERS, timeout=30)\n",
    "            time.sleep(5 + 10*random.random())\n",
    "            r.raise_for_status()\n",
    "\n",
    "            # 2) parse title + lyrics\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            # Title: bold in center column; fallback to <title>\n",
    "            center = soup.select_one(\"div.col-xs-12.col-lg-8.text-center\")\n",
    "            title = (center.select_one(\"b\").get_text(strip=True).strip('\"')\n",
    "                     if center and center.select_one(\"b\")\n",
    "                     else (soup.title.get_text(strip=True) if soup.title else \"Unknown Title\"))\n",
    "\n",
    "            # Lyrics: first child <div> with no attributes inside center column\n",
    "            lyrics = \"\"\n",
    "            if center:\n",
    "                for child in center.find_all(\"div\", recursive=False):\n",
    "                    if not child.attrs:\n",
    "                        lyrics = child.get_text(\"\\n\", strip=True)\n",
    "                        break\n",
    "\n",
    "            # Fallback: any no-attr div\n",
    "            if not lyrics:\n",
    "                cand = soup.find(\"div\", attrs={})\n",
    "                lyrics = cand.get_text(\"\\n\", strip=True) if cand else \"\"\n",
    "\n",
    "            if not lyrics:\n",
    "                continue  # skip blanks/blocked pages\n",
    "\n",
    "            # 3) write to file\n",
    "            filename = generate_filename_from_link(link)\n",
    "            with open(os.path.join(artist_folder, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"{title}\\n\\n{lyrics}\")\n",
    "\n",
    "            saved += 1\n",
    "            total_pages += 1\n",
    "\n",
    "            # keep minimum requirement; remove this to save ALL songs\n",
    "            if saved >= 20:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipped {link}: {e}\")\n",
    "\n",
    "    print(f\"Saved {saved} files for {artist}\")\n",
    "\n",
    "print(f\"\\nScraping complete: {total_pages} files written.\")\n",
    "print(f\"Total runtime: {round((time.time()-start)/60, 1)} minutes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20341150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time was 0.12 hours.\n"
     ]
    }
   ],
   "source": [
    "# Total Run Time\n",
    "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28b458",
   "metadata": {},
   "source": [
    "# Evaulation\n",
    "This section summarizes how many files were created per artist \n",
    "and counts the total and unique words in their lyrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ee5861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulation \n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b93ea01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rush we have 20 files.\n",
      "For rush we have roughly 5164 words, 1112 are unique.\n",
      "For the_who we have 20 files.\n",
      "For the_who we have roughly 4995 words, 914 are unique.\n"
     ]
    }
   ],
   "source": [
    "# Check Lyrics \n",
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "for artist in artist_folders : \n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {artist} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "\n",
    "    for f_name in artist_files : \n",
    "        with open(\"lyrics/\" + artist + \"/\" + f_name) as infile : \n",
    "            artist_words.extend(words(infile.read()))\n",
    "\n",
    "            \n",
    "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e26257",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Both artists had sufficient songs, the scraper saved 20 lyrics files per artist, \n",
    "and the evaluation confirmed the output contained thousands of words \n",
    "with high vocabulary diversity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
